# pip install beautifulsoup4 langchain_community langchain streamlit ollama faiss-gpu pypdf

# https://anakin.ai/blog/llama-3-rag-locally/

import os
os.environ['USER_AGENT'] = 'myagent'
import random
from glob import glob
import streamlit as st
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (WebBaseLoader,
                                                  TextLoader,
                                                  PyPDFLoader,
                                                  MergedDataLoader,
                                                  SeleniumURLLoader,
                                                  WikipediaLoader,
                                                 )
from langchain_core.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings

def ollama_llm(question, context):
    formatted_prompt = f'''Context: {context}
        Based on the context, answer the following question:
        Question: {question}\n
        {userpart_system_promt}''' # f"Question: {question}\n\nContext: {context}"
    
    response = ollama.chat(model=llmodel, 
                           messages=[{'role': 'user', 'content': formatted_prompt}],
                           options={
                                    'temperature': temperatuur, # –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0,0 –¥–æ 0,9 (–∏–ª–∏ 1) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –µ–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
                                    #'top_p': 0.9, #  –æ—Ç 0,1 –¥–æ 0,9 –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –≤—ã–±—Ä–∞—Ç—å, –∏—Å—Ö–æ–¥—è –∏–∑ –∏—Ö —Å–æ–≤–æ–∫—É–ø–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.
                                    #'top_k': 90, # –æ—Ç 1 –¥–æ 100 –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∏–∑ —Å–∫–æ–ª—å–∫–∏—Ö –ª–µ–∫—Å–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏) –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –≤—ã–±—Ä–∞—Ç—å, —á—Ç–æ–±—ã –≤—ã–¥–∞—Ç—å –æ—Ç–≤–µ—Ç.
                                    #'num_ctx': 500_000, # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ –æ–±–ª–∞—Å—Ç—å—é –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.
                                    'num_predict': predict_msg, # –∑–∞–¥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–∞—Ö –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è (100 tokens ~ 75 words).
                                    },
                        )
    return response['message']['content']

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# def rag_chain(question):
#     search = vectorstore.similarity_search(question, k=5)
#     temp_prompt = PromptTemplate(input_variables=["context", "question"], template=template)
#     final_prompt = prompt.format(question=query, context=search)
#     retrieved_docs = retriever.invoke(question)
#     formatted_context = combine_docs(retrieved_docs)
#     return ollama_llm(question, formatted_context)


st.title("LLM's Chat: üåê Web & üìù Docs")
st.caption("–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –∏—Å–ø–æ–ª—å–∑—É—â–∏–π RAG-–º–µ—Ç–æ–¥ –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM.") # deepseek-r1:7b (arch:qwen2)

llmodel = st.selectbox(
                '–í—ã–±–µ—Ä–∏—Ç–µ LLM:',
                    ('deepseek-r1:32b',
                     'deepseek-r1:7b',
                     #'qwen2.5-coder:32b', 
                     'llama3.3', 
                     'llama3.2', 
                     'mistral-small', 
                     #'lakomoor/vikhr-llama-3.2-1b-instruct:1b', 
                     #'rscr/vikhr_nemo_12b', 
                     #'hf.co/yasserrmd/Qwen2.5-7B-Instruct-1M-gguf:Q4_K_M'
                    )
)

system_promt = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç üëá (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)", 
                              help='''–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —á—Ç–æ –æ—Ç –Ω–µ–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.
                              –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ –≤–∞—Ä–∏–∞–Ω—Ç: –æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É''')

if system_promt:
    userpart_system_promt = system_promt
else:
    userpart_system_promt = '''Provide an answer based only on the context provided, without using general knowledge. The answer should be taken directly from the context provided.
        Please correct grammatical errors to improve readability.
        If there is not enough information in the context to answer the question, indicate that the answer is missing in this context.
        Please include the source of the information as a link explaining how you came to your answer.'''

temperatuur = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM)", 0.0, 2.0, step=0.1, value=0.1)

predict_msg = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –¥–ª–∏–Ω—É –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞ –ø–µ—á–∞—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (tokens): 100 tokens = 75 words in English",
                         0, 
                         100_000, 
                         step=100, 
                         value=3000,
                        )
chunk_size = st.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—É—Å–∫–∞ (tokens)", 100, 2000, step=100, value=1200)
chunk_overlap = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–µ—Ä–µ—Å–µ–∫–∞—Ç—å—Å—è –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –∫—É—Å–∫–∞–º–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)", 0, 1000, step=25, value=300)

# Get the webpage URL from the user
webpage_url = st.text_area("–î–æ–±–∞–≤—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–µ Web-—Å—Ç—Ä–∞–Ω–∏—Ü—ã (URLs)", help='–ú–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ "Enter"')

document_files_path = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ txt –∏–ª–∏ pdf —Ñ–∞–π–ª—ã", 
                                          type=["txt", "pdf"],
                                          help="Upload a PDF or TXT file",
                                          key="file_uploader",
                                          accept_multiple_files=True)
unic_key = 0
question = st.text_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É web-—Å—Ç—Ä–∞–Ω–∏—Ü(—ã) –∏/–∏–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º", key=f'{unic_key}', help='–î–ª—è —Å—Ç–∞—Ä—Ç–∞ –Ω–∞–∑–º–∏—Ç–µ "Enter"')

# –®–∞–±–ª–æ–Ω –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
template = "–í–æ–ø—Ä–æ—Å: {question} \n\n–û—Ç–≤–µ—Ç:"


web_loader, txt_loader, pdf_loader = None, None, None
document_file_path = []

if question:
    print(webpage_url, web_loader)
    # 1. Load the data
    if document_files_path:

        for document_file_path in document_files_path:         

            if document_file_path.name[-3:] == 'txt':
                temp_file_txt = f"./temp_{random.randint(1, 1_000_000)}.txt"
                with open(temp_file_txt, "ab") as file:
                    file.write(document_file_path.getvalue())
                txt_loader = TextLoader(temp_file_txt)

            if document_file_path.name[-3:] == 'pdf':
                temp_file_pdf = f"./temp_{random.randint(1, 1_000_000)}.pdf"
                with open(temp_file_pdf, "ab") as file:
                    file.write(document_file_path.getvalue())
                pdf_loader = PyPDFLoader(temp_file_pdf, extract_images = False)

    if webpage_url == '':
        webpage_url = 'http://localhost:8501'
        web_loader = WebBaseLoader(webpage_url)
    else:
        web_loader = WebBaseLoader(webpage_url,encoding='utf-8')
        st.success(f"Loaded {webpage_url} successfully!")
    print(webpage_url, web_loader)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤
    if (txt_loader and pdf_loader) is not None:
        merged_loader = MergedDataLoader(loaders=[web_loader, txt_loader, pdf_loader])
        print('merged txt, pdf')
    elif txt_loader is not None:
        merged_loader = MergedDataLoader(loaders=[web_loader, txt_loader])
        print('merged txt')
    elif pdf_loader is not None:
        merged_loader = MergedDataLoader(loaders=[web_loader, pdf_loader])
        print('merged pdf')
    else:
        merged_loader = MergedDataLoader(loaders=[web_loader])
        print('merged web')

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    docs = merged_loader.load()
    
    print(docs)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap, is_separator_regex = True)
    splits = text_splitter.split_documents(docs)
    
    # 2. Create Ollama embeddings and vector store
    embeddings = OllamaEmbeddings(model=llmodel)
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

    vectorstore.save_local("faiss_AiDocs")

    vectorstore = FAISS.load_local("faiss_AiDocs", embeddings, allow_dangerous_deserialization=True)

    # 4. RAG Setup
    retriever = vectorstore.as_retriever()

    # Ask a question about the webpage
    # Chat with the webpage
    if question:
        # result = rag_chain(prompt)

        search = vectorstore.similarity_search(question, k=5)
        prompt = PromptTemplate(input_variables=["context", "question"], template=template)

        final_prompt = prompt.format(question=question, context=search)
        retrieved_docs = retriever.invoke(final_prompt)
        formatted_context = combine_docs(retrieved_docs)

        result = ollama_llm(question, formatted_context)
        st.write(result)
        unic_key += 1

        print(os.getcwd())

        print(glob(f"{os.getcwd()}/*.txt"))
        print(glob(f"{os.getcwd()}/*.pdf"))

        # —É–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            for file in glob(f"{os.getcwd()}/*.txt"):
                os.remove(file)
        except OSError:
            pass

        try:
            for file in glob(f"{os.getcwd()}/*.pdf"):
                os.remove(file)
        except OSError:
            pass

        print('hier')


# streamlit run app.py
